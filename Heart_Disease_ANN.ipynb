{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\bryan\\Desktop\\Heart_Disease_App\\heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
       "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
       "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
       "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
       "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   2     3       0  \n",
       "1   0     3       0  \n",
       "2   0     3       0  \n",
       "3   1     3       0  \n",
       "4   3     2       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "1020   59    1   1       140   221    0        1      164      1      0.0   \n",
       "1021   60    1   0       125   258    0        0      141      1      2.8   \n",
       "1022   47    1   0       110   275    0        0      118      1      1.0   \n",
       "1023   50    0   0       110   254    0        0      159      0      0.0   \n",
       "1024   54    1   0       120   188    0        1      113      0      1.4   \n",
       "\n",
       "      slope  ca  thal  target  \n",
       "1020      2   0     2       1  \n",
       "1021      1   1     3       0  \n",
       "1022      1   1     2       0  \n",
       "1023      2   0     2       1  \n",
       "1024      1   1     3       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset courtesy of Kaggle: https://www.kaggle.com/ronitf/heart-disease-uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target.values\n",
    "X = df.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    526\n",
       "0    499\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"target\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data for better predictions\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 686 samples, validate on 339 samples\n",
      "Epoch 1/1000\n",
      "686/686 [==============================] - 1s 2ms/sample - loss: 0.6862 - accuracy: 0.5773 - val_loss: 0.6127 - val_accuracy: 0.7198\n",
      "Epoch 2/1000\n",
      "686/686 [==============================] - 0s 95us/sample - loss: 0.6020 - accuracy: 0.6851 - val_loss: 0.5359 - val_accuracy: 0.7611\n",
      "Epoch 3/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.5378 - accuracy: 0.7566 - val_loss: 0.4784 - val_accuracy: 0.7906\n",
      "Epoch 4/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.4943 - accuracy: 0.7813 - val_loss: 0.4366 - val_accuracy: 0.8171\n",
      "Epoch 5/1000\n",
      "686/686 [==============================] - 0s 112us/sample - loss: 0.4460 - accuracy: 0.8120 - val_loss: 0.4055 - val_accuracy: 0.8319\n",
      "Epoch 6/1000\n",
      "686/686 [==============================] - 0s 106us/sample - loss: 0.4332 - accuracy: 0.8324 - val_loss: 0.3849 - val_accuracy: 0.8230\n",
      "Epoch 7/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.4045 - accuracy: 0.8251 - val_loss: 0.3725 - val_accuracy: 0.8319\n",
      "Epoch 8/1000\n",
      "686/686 [==============================] - 0s 100us/sample - loss: 0.3735 - accuracy: 0.8207 - val_loss: 0.3620 - val_accuracy: 0.8407\n",
      "Epoch 9/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.4001 - accuracy: 0.8338 - val_loss: 0.3538 - val_accuracy: 0.8407\n",
      "Epoch 10/1000\n",
      "686/686 [==============================] - 0s 99us/sample - loss: 0.3809 - accuracy: 0.8324 - val_loss: 0.3477 - val_accuracy: 0.8496\n",
      "Epoch 11/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.3504 - accuracy: 0.8499 - val_loss: 0.3426 - val_accuracy: 0.8348\n",
      "Epoch 12/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.3722 - accuracy: 0.8440 - val_loss: 0.3390 - val_accuracy: 0.8496\n",
      "Epoch 13/1000\n",
      "686/686 [==============================] - 0s 112us/sample - loss: 0.3588 - accuracy: 0.8411 - val_loss: 0.3346 - val_accuracy: 0.8555\n",
      "Epoch 14/1000\n",
      "686/686 [==============================] - 0s 100us/sample - loss: 0.3421 - accuracy: 0.8557 - val_loss: 0.3304 - val_accuracy: 0.8584\n",
      "Epoch 15/1000\n",
      "686/686 [==============================] - 0s 113us/sample - loss: 0.3321 - accuracy: 0.8673 - val_loss: 0.3257 - val_accuracy: 0.8555\n",
      "Epoch 16/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.3193 - accuracy: 0.8761 - val_loss: 0.3242 - val_accuracy: 0.8584\n",
      "Epoch 17/1000\n",
      "686/686 [==============================] - 0s 102us/sample - loss: 0.3298 - accuracy: 0.8557 - val_loss: 0.3217 - val_accuracy: 0.8584\n",
      "Epoch 18/1000\n",
      "686/686 [==============================] - 0s 99us/sample - loss: 0.3112 - accuracy: 0.8848 - val_loss: 0.3185 - val_accuracy: 0.8584\n",
      "Epoch 19/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.3119 - accuracy: 0.8761 - val_loss: 0.3169 - val_accuracy: 0.8614\n",
      "Epoch 20/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.2920 - accuracy: 0.8863 - val_loss: 0.3143 - val_accuracy: 0.8673\n",
      "Epoch 21/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.3026 - accuracy: 0.8805 - val_loss: 0.3104 - val_accuracy: 0.8673\n",
      "Epoch 22/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.2876 - accuracy: 0.8834 - val_loss: 0.3081 - val_accuracy: 0.8702\n",
      "Epoch 23/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.3075 - accuracy: 0.8863 - val_loss: 0.3070 - val_accuracy: 0.8732\n",
      "Epoch 24/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.2757 - accuracy: 0.8776 - val_loss: 0.3045 - val_accuracy: 0.8702\n",
      "Epoch 25/1000\n",
      "686/686 [==============================] - 0s 110us/sample - loss: 0.3034 - accuracy: 0.8863 - val_loss: 0.3029 - val_accuracy: 0.8702\n",
      "Epoch 26/1000\n",
      "686/686 [==============================] - 0s 99us/sample - loss: 0.2920 - accuracy: 0.8834 - val_loss: 0.3001 - val_accuracy: 0.8732\n",
      "Epoch 27/1000\n",
      "686/686 [==============================] - 0s 99us/sample - loss: 0.2690 - accuracy: 0.8950 - val_loss: 0.2978 - val_accuracy: 0.8732\n",
      "Epoch 28/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.2703 - accuracy: 0.8892 - val_loss: 0.2963 - val_accuracy: 0.8761\n",
      "Epoch 29/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.2653 - accuracy: 0.9023 - val_loss: 0.2927 - val_accuracy: 0.8761\n",
      "Epoch 30/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.2946 - accuracy: 0.8761 - val_loss: 0.2899 - val_accuracy: 0.8673\n",
      "Epoch 31/1000\n",
      "686/686 [==============================] - 0s 99us/sample - loss: 0.2665 - accuracy: 0.8965 - val_loss: 0.2866 - val_accuracy: 0.8850\n",
      "Epoch 32/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.2627 - accuracy: 0.9096 - val_loss: 0.2827 - val_accuracy: 0.8791\n",
      "Epoch 33/1000\n",
      "686/686 [==============================] - 0s 100us/sample - loss: 0.2728 - accuracy: 0.8761 - val_loss: 0.2801 - val_accuracy: 0.8879\n",
      "Epoch 34/1000\n",
      "686/686 [==============================] - 0s 113us/sample - loss: 0.2457 - accuracy: 0.9111 - val_loss: 0.2776 - val_accuracy: 0.8879\n",
      "Epoch 35/1000\n",
      "686/686 [==============================] - 0s 99us/sample - loss: 0.2778 - accuracy: 0.8907 - val_loss: 0.2768 - val_accuracy: 0.8938\n",
      "Epoch 36/1000\n",
      "686/686 [==============================] - 0s 102us/sample - loss: 0.2529 - accuracy: 0.8980 - val_loss: 0.2749 - val_accuracy: 0.8820\n",
      "Epoch 37/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.2408 - accuracy: 0.8980 - val_loss: 0.2714 - val_accuracy: 0.8850\n",
      "Epoch 38/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.2439 - accuracy: 0.8950 - val_loss: 0.2697 - val_accuracy: 0.8850\n",
      "Epoch 39/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.2479 - accuracy: 0.8980 - val_loss: 0.2697 - val_accuracy: 0.8820\n",
      "Epoch 40/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.2548 - accuracy: 0.8834 - val_loss: 0.2684 - val_accuracy: 0.8879\n",
      "Epoch 41/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.2307 - accuracy: 0.9009 - val_loss: 0.2670 - val_accuracy: 0.8850\n",
      "Epoch 42/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.2493 - accuracy: 0.8907 - val_loss: 0.2654 - val_accuracy: 0.8791\n",
      "Epoch 43/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.2490 - accuracy: 0.8921 - val_loss: 0.2645 - val_accuracy: 0.8968\n",
      "Epoch 44/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.2272 - accuracy: 0.8994 - val_loss: 0.2621 - val_accuracy: 0.8909\n",
      "Epoch 45/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.2427 - accuracy: 0.8980 - val_loss: 0.2606 - val_accuracy: 0.8879\n",
      "Epoch 46/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.2417 - accuracy: 0.9067 - val_loss: 0.2608 - val_accuracy: 0.8879\n",
      "Epoch 47/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.2203 - accuracy: 0.9140 - val_loss: 0.2574 - val_accuracy: 0.8850\n",
      "Epoch 48/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.2567 - accuracy: 0.8936 - val_loss: 0.2575 - val_accuracy: 0.8820\n",
      "Epoch 49/1000\n",
      "686/686 [==============================] - 0s 95us/sample - loss: 0.2420 - accuracy: 0.8950 - val_loss: 0.2549 - val_accuracy: 0.8850\n",
      "Epoch 50/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.2313 - accuracy: 0.9111 - val_loss: 0.2516 - val_accuracy: 0.8879\n",
      "Epoch 51/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.2312 - accuracy: 0.8994 - val_loss: 0.2522 - val_accuracy: 0.8850\n",
      "Epoch 52/1000\n",
      "686/686 [==============================] - 0s 95us/sample - loss: 0.2236 - accuracy: 0.9125 - val_loss: 0.2508 - val_accuracy: 0.8820\n",
      "Epoch 53/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.2138 - accuracy: 0.9111 - val_loss: 0.2476 - val_accuracy: 0.8820\n",
      "Epoch 54/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.2232 - accuracy: 0.9009 - val_loss: 0.2468 - val_accuracy: 0.8850\n",
      "Epoch 55/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.2170 - accuracy: 0.9155 - val_loss: 0.2468 - val_accuracy: 0.8820\n",
      "Epoch 56/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.2018 - accuracy: 0.9140 - val_loss: 0.2482 - val_accuracy: 0.8938\n",
      "Epoch 57/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.2003 - accuracy: 0.9227 - val_loss: 0.2462 - val_accuracy: 0.8938\n",
      "Epoch 58/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.2055 - accuracy: 0.9125 - val_loss: 0.2435 - val_accuracy: 0.8938\n",
      "Epoch 59/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.2160 - accuracy: 0.9140 - val_loss: 0.2411 - val_accuracy: 0.9027\n",
      "Epoch 60/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.2181 - accuracy: 0.9009 - val_loss: 0.2399 - val_accuracy: 0.8938\n",
      "Epoch 61/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1992 - accuracy: 0.9315 - val_loss: 0.2389 - val_accuracy: 0.8850\n",
      "Epoch 62/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.2129 - accuracy: 0.9169 - val_loss: 0.2388 - val_accuracy: 0.8850\n",
      "Epoch 63/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1943 - accuracy: 0.9198 - val_loss: 0.2370 - val_accuracy: 0.8938\n",
      "Epoch 64/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.2085 - accuracy: 0.9198 - val_loss: 0.2363 - val_accuracy: 0.8938\n",
      "Epoch 65/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.2051 - accuracy: 0.9257 - val_loss: 0.2330 - val_accuracy: 0.9027\n",
      "Epoch 66/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1942 - accuracy: 0.9242 - val_loss: 0.2296 - val_accuracy: 0.9056\n",
      "Epoch 67/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.1918 - accuracy: 0.9242 - val_loss: 0.2287 - val_accuracy: 0.9027\n",
      "Epoch 68/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1894 - accuracy: 0.9300 - val_loss: 0.2258 - val_accuracy: 0.9027\n",
      "Epoch 69/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1810 - accuracy: 0.9315 - val_loss: 0.2273 - val_accuracy: 0.9027\n",
      "Epoch 70/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.1965 - accuracy: 0.9286 - val_loss: 0.2237 - val_accuracy: 0.8997\n",
      "Epoch 71/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1897 - accuracy: 0.9271 - val_loss: 0.2198 - val_accuracy: 0.9115\n",
      "Epoch 72/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1934 - accuracy: 0.9184 - val_loss: 0.2196 - val_accuracy: 0.9056\n",
      "Epoch 73/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1741 - accuracy: 0.9504 - val_loss: 0.2174 - val_accuracy: 0.9115\n",
      "Epoch 74/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.1964 - accuracy: 0.9257 - val_loss: 0.2134 - val_accuracy: 0.9174\n",
      "Epoch 75/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.1782 - accuracy: 0.9461 - val_loss: 0.2119 - val_accuracy: 0.9115\n",
      "Epoch 76/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1792 - accuracy: 0.9329 - val_loss: 0.2093 - val_accuracy: 0.9115\n",
      "Epoch 77/1000\n",
      "686/686 [==============================] - 0s 80us/sample - loss: 0.1854 - accuracy: 0.9329 - val_loss: 0.2068 - val_accuracy: 0.9115\n",
      "Epoch 78/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1642 - accuracy: 0.9329 - val_loss: 0.2069 - val_accuracy: 0.9115\n",
      "Epoch 79/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1578 - accuracy: 0.9373 - val_loss: 0.2056 - val_accuracy: 0.9115\n",
      "Epoch 80/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1712 - accuracy: 0.9329 - val_loss: 0.2019 - val_accuracy: 0.9174\n",
      "Epoch 81/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1725 - accuracy: 0.9373 - val_loss: 0.2022 - val_accuracy: 0.9115\n",
      "Epoch 82/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1667 - accuracy: 0.9359 - val_loss: 0.1992 - val_accuracy: 0.9027\n",
      "Epoch 83/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1602 - accuracy: 0.9431 - val_loss: 0.1996 - val_accuracy: 0.9115\n",
      "Epoch 84/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.1694 - accuracy: 0.9359 - val_loss: 0.1981 - val_accuracy: 0.9174\n",
      "Epoch 85/1000\n",
      "686/686 [==============================] - 0s 105us/sample - loss: 0.1602 - accuracy: 0.9373 - val_loss: 0.1983 - val_accuracy: 0.9174\n",
      "Epoch 86/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1723 - accuracy: 0.9329 - val_loss: 0.1987 - val_accuracy: 0.9027\n",
      "Epoch 87/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.1635 - accuracy: 0.9402 - val_loss: 0.1949 - val_accuracy: 0.9027\n",
      "Epoch 88/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1519 - accuracy: 0.9475 - val_loss: 0.1915 - val_accuracy: 0.9174\n",
      "Epoch 89/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1902 - accuracy: 0.9271 - val_loss: 0.1925 - val_accuracy: 0.9115\n",
      "Epoch 90/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1627 - accuracy: 0.9286 - val_loss: 0.1929 - val_accuracy: 0.9174\n",
      "Epoch 91/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1672 - accuracy: 0.9402 - val_loss: 0.1927 - val_accuracy: 0.9233\n",
      "Epoch 92/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.1621 - accuracy: 0.9461 - val_loss: 0.1919 - val_accuracy: 0.9174\n",
      "Epoch 93/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1558 - accuracy: 0.9446 - val_loss: 0.1890 - val_accuracy: 0.9233\n",
      "Epoch 94/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1469 - accuracy: 0.9446 - val_loss: 0.1870 - val_accuracy: 0.9204\n",
      "Epoch 95/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1412 - accuracy: 0.9446 - val_loss: 0.1883 - val_accuracy: 0.9204\n",
      "Epoch 96/1000\n",
      "686/686 [==============================] - 0s 94us/sample - loss: 0.1320 - accuracy: 0.9534 - val_loss: 0.1860 - val_accuracy: 0.9204\n",
      "Epoch 97/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1338 - accuracy: 0.9519 - val_loss: 0.1855 - val_accuracy: 0.9204\n",
      "Epoch 98/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1481 - accuracy: 0.9461 - val_loss: 0.1842 - val_accuracy: 0.9174\n",
      "Epoch 99/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.1625 - accuracy: 0.9373 - val_loss: 0.1823 - val_accuracy: 0.9204\n",
      "Epoch 100/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.1423 - accuracy: 0.9548 - val_loss: 0.1789 - val_accuracy: 0.9204\n",
      "Epoch 101/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1472 - accuracy: 0.9329 - val_loss: 0.1781 - val_accuracy: 0.9174\n",
      "Epoch 102/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1419 - accuracy: 0.9490 - val_loss: 0.1787 - val_accuracy: 0.9233\n",
      "Epoch 103/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1361 - accuracy: 0.9504 - val_loss: 0.1768 - val_accuracy: 0.9263\n",
      "Epoch 104/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1611 - accuracy: 0.9329 - val_loss: 0.1763 - val_accuracy: 0.9204\n",
      "Epoch 105/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1415 - accuracy: 0.9475 - val_loss: 0.1781 - val_accuracy: 0.9115\n",
      "Epoch 106/1000\n",
      "686/686 [==============================] - 0s 91us/sample - loss: 0.1382 - accuracy: 0.9475 - val_loss: 0.1769 - val_accuracy: 0.9115\n",
      "Epoch 107/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1483 - accuracy: 0.9461 - val_loss: 0.1780 - val_accuracy: 0.9204\n",
      "Epoch 108/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1385 - accuracy: 0.9402 - val_loss: 0.1766 - val_accuracy: 0.9233\n",
      "Epoch 109/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.1590 - accuracy: 0.9286 - val_loss: 0.1734 - val_accuracy: 0.9204\n",
      "Epoch 110/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1471 - accuracy: 0.9431 - val_loss: 0.1744 - val_accuracy: 0.9204\n",
      "Epoch 111/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1530 - accuracy: 0.9490 - val_loss: 0.1725 - val_accuracy: 0.9204\n",
      "Epoch 112/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1457 - accuracy: 0.9402 - val_loss: 0.1713 - val_accuracy: 0.9233\n",
      "Epoch 113/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.1455 - accuracy: 0.9402 - val_loss: 0.1690 - val_accuracy: 0.9145\n",
      "Epoch 114/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1278 - accuracy: 0.9563 - val_loss: 0.1688 - val_accuracy: 0.9145\n",
      "Epoch 115/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1444 - accuracy: 0.9475 - val_loss: 0.1660 - val_accuracy: 0.9292\n",
      "Epoch 116/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1484 - accuracy: 0.9461 - val_loss: 0.1639 - val_accuracy: 0.9292\n",
      "Epoch 117/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1415 - accuracy: 0.9446 - val_loss: 0.1642 - val_accuracy: 0.9292\n",
      "Epoch 118/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1374 - accuracy: 0.9388 - val_loss: 0.1644 - val_accuracy: 0.9292\n",
      "Epoch 119/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1436 - accuracy: 0.9461 - val_loss: 0.1640 - val_accuracy: 0.9204\n",
      "Epoch 120/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.1376 - accuracy: 0.9563 - val_loss: 0.1641 - val_accuracy: 0.9204\n",
      "Epoch 121/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1454 - accuracy: 0.9519 - val_loss: 0.1600 - val_accuracy: 0.9204\n",
      "Epoch 122/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1382 - accuracy: 0.9548 - val_loss: 0.1588 - val_accuracy: 0.9204\n",
      "Epoch 123/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1402 - accuracy: 0.9431 - val_loss: 0.1591 - val_accuracy: 0.9292\n",
      "Epoch 124/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.1363 - accuracy: 0.9504 - val_loss: 0.1573 - val_accuracy: 0.9233\n",
      "Epoch 125/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.1229 - accuracy: 0.9519 - val_loss: 0.1577 - val_accuracy: 0.9322\n",
      "Epoch 126/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1365 - accuracy: 0.9417 - val_loss: 0.1558 - val_accuracy: 0.9292\n",
      "Epoch 127/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1444 - accuracy: 0.9504 - val_loss: 0.1544 - val_accuracy: 0.9322\n",
      "Epoch 128/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1397 - accuracy: 0.9490 - val_loss: 0.1519 - val_accuracy: 0.9322\n",
      "Epoch 129/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1232 - accuracy: 0.9621 - val_loss: 0.1498 - val_accuracy: 0.9322\n",
      "Epoch 130/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1092 - accuracy: 0.9563 - val_loss: 0.1522 - val_accuracy: 0.9322\n",
      "Epoch 131/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1517 - accuracy: 0.9388 - val_loss: 0.1489 - val_accuracy: 0.9322\n",
      "Epoch 132/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1153 - accuracy: 0.9592 - val_loss: 0.1485 - val_accuracy: 0.9322\n",
      "Epoch 133/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1216 - accuracy: 0.9592 - val_loss: 0.1483 - val_accuracy: 0.9322\n",
      "Epoch 134/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.1410 - accuracy: 0.9490 - val_loss: 0.1497 - val_accuracy: 0.9322\n",
      "Epoch 135/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1091 - accuracy: 0.9534 - val_loss: 0.1456 - val_accuracy: 0.9322\n",
      "Epoch 136/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1283 - accuracy: 0.9592 - val_loss: 0.1445 - val_accuracy: 0.9322\n",
      "Epoch 137/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1201 - accuracy: 0.9519 - val_loss: 0.1447 - val_accuracy: 0.9322\n",
      "Epoch 138/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1284 - accuracy: 0.9461 - val_loss: 0.1432 - val_accuracy: 0.9322\n",
      "Epoch 139/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.1328 - accuracy: 0.9446 - val_loss: 0.1452 - val_accuracy: 0.9322\n",
      "Epoch 140/1000\n",
      "686/686 [==============================] - 0s 95us/sample - loss: 0.1198 - accuracy: 0.9519 - val_loss: 0.1455 - val_accuracy: 0.9322\n",
      "Epoch 141/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1274 - accuracy: 0.9519 - val_loss: 0.1458 - val_accuracy: 0.9322\n",
      "Epoch 142/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1365 - accuracy: 0.9534 - val_loss: 0.1484 - val_accuracy: 0.9292\n",
      "Epoch 143/1000\n",
      "686/686 [==============================] - 0s 91us/sample - loss: 0.1300 - accuracy: 0.9577 - val_loss: 0.1493 - val_accuracy: 0.9233\n",
      "Epoch 144/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1160 - accuracy: 0.9490 - val_loss: 0.1480 - val_accuracy: 0.9292\n",
      "Epoch 145/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1331 - accuracy: 0.9519 - val_loss: 0.1465 - val_accuracy: 0.9233\n",
      "Epoch 146/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.1089 - accuracy: 0.9606 - val_loss: 0.1466 - val_accuracy: 0.9233\n",
      "Epoch 147/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.1130 - accuracy: 0.9636 - val_loss: 0.1459 - val_accuracy: 0.9233\n",
      "Epoch 148/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1279 - accuracy: 0.9490 - val_loss: 0.1427 - val_accuracy: 0.9263\n",
      "Epoch 149/1000\n",
      "686/686 [==============================] - 0s 103us/sample - loss: 0.1111 - accuracy: 0.9577 - val_loss: 0.1384 - val_accuracy: 0.9263\n",
      "Epoch 150/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1139 - accuracy: 0.9534 - val_loss: 0.1398 - val_accuracy: 0.9263\n",
      "Epoch 151/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0922 - accuracy: 0.9679 - val_loss: 0.1395 - val_accuracy: 0.9322\n",
      "Epoch 152/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.1152 - accuracy: 0.9519 - val_loss: 0.1389 - val_accuracy: 0.9322\n",
      "Epoch 153/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1057 - accuracy: 0.9606 - val_loss: 0.1369 - val_accuracy: 0.9381\n",
      "Epoch 154/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1173 - accuracy: 0.9548 - val_loss: 0.1387 - val_accuracy: 0.9351\n",
      "Epoch 155/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1194 - accuracy: 0.9519 - val_loss: 0.1411 - val_accuracy: 0.9292\n",
      "Epoch 156/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.1140 - accuracy: 0.9606 - val_loss: 0.1401 - val_accuracy: 0.9233\n",
      "Epoch 157/1000\n",
      "686/686 [==============================] - 0s 102us/sample - loss: 0.1134 - accuracy: 0.9592 - val_loss: 0.1327 - val_accuracy: 0.9351\n",
      "Epoch 158/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1221 - accuracy: 0.9563 - val_loss: 0.1326 - val_accuracy: 0.9263\n",
      "Epoch 159/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.1114 - accuracy: 0.9534 - val_loss: 0.1366 - val_accuracy: 0.9351\n",
      "Epoch 160/1000\n",
      "686/686 [==============================] - 0s 100us/sample - loss: 0.1299 - accuracy: 0.9475 - val_loss: 0.1365 - val_accuracy: 0.9351\n",
      "Epoch 161/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.1137 - accuracy: 0.9577 - val_loss: 0.1393 - val_accuracy: 0.9322\n",
      "Epoch 162/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1315 - accuracy: 0.9490 - val_loss: 0.1407 - val_accuracy: 0.9322\n",
      "Epoch 163/1000\n",
      "686/686 [==============================] - 0s 96us/sample - loss: 0.1225 - accuracy: 0.9577 - val_loss: 0.1385 - val_accuracy: 0.9322\n",
      "Epoch 164/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1020 - accuracy: 0.9636 - val_loss: 0.1350 - val_accuracy: 0.9322\n",
      "Epoch 165/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0873 - accuracy: 0.9621 - val_loss: 0.1342 - val_accuracy: 0.9322\n",
      "Epoch 166/1000\n",
      "686/686 [==============================] - 0s 95us/sample - loss: 0.1101 - accuracy: 0.9665 - val_loss: 0.1319 - val_accuracy: 0.9381\n",
      "Epoch 167/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1171 - accuracy: 0.9577 - val_loss: 0.1271 - val_accuracy: 0.9381\n",
      "Epoch 168/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0950 - accuracy: 0.9606 - val_loss: 0.1274 - val_accuracy: 0.9381\n",
      "Epoch 169/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.1070 - accuracy: 0.9606 - val_loss: 0.1286 - val_accuracy: 0.9381\n",
      "Epoch 170/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1048 - accuracy: 0.9650 - val_loss: 0.1281 - val_accuracy: 0.9410\n",
      "Epoch 171/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0960 - accuracy: 0.9650 - val_loss: 0.1267 - val_accuracy: 0.9469\n",
      "Epoch 172/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0889 - accuracy: 0.9621 - val_loss: 0.1281 - val_accuracy: 0.9381\n",
      "Epoch 173/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1182 - accuracy: 0.9548 - val_loss: 0.1234 - val_accuracy: 0.9440\n",
      "Epoch 174/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 1.00 - 0s 87us/sample - loss: 0.1115 - accuracy: 0.9577 - val_loss: 0.1246 - val_accuracy: 0.9410\n",
      "Epoch 175/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1274 - accuracy: 0.9534 - val_loss: 0.1270 - val_accuracy: 0.9499\n",
      "Epoch 176/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.1015 - accuracy: 0.9621 - val_loss: 0.1274 - val_accuracy: 0.9499\n",
      "Epoch 177/1000\n",
      "686/686 [==============================] - 0s 124us/sample - loss: 0.0884 - accuracy: 0.9694 - val_loss: 0.1236 - val_accuracy: 0.9351\n",
      "Epoch 178/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1140 - accuracy: 0.9475 - val_loss: 0.1247 - val_accuracy: 0.9351\n",
      "Epoch 179/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0858 - accuracy: 0.9694 - val_loss: 0.1287 - val_accuracy: 0.9351\n",
      "Epoch 180/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1016 - accuracy: 0.9592 - val_loss: 0.1220 - val_accuracy: 0.9351\n",
      "Epoch 181/1000\n",
      "686/686 [==============================] - 0s 100us/sample - loss: 0.0931 - accuracy: 0.9665 - val_loss: 0.1179 - val_accuracy: 0.9351\n",
      "Epoch 182/1000\n",
      "686/686 [==============================] - 0s 106us/sample - loss: 0.1022 - accuracy: 0.9563 - val_loss: 0.1165 - val_accuracy: 0.9351\n",
      "Epoch 183/1000\n",
      "686/686 [==============================] - 0s 105us/sample - loss: 0.0809 - accuracy: 0.9752 - val_loss: 0.1173 - val_accuracy: 0.9410\n",
      "Epoch 184/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.0926 - accuracy: 0.9665 - val_loss: 0.1168 - val_accuracy: 0.9410\n",
      "Epoch 185/1000\n",
      "686/686 [==============================] - 0s 96us/sample - loss: 0.0997 - accuracy: 0.9621 - val_loss: 0.1160 - val_accuracy: 0.9440\n",
      "Epoch 186/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.1019 - accuracy: 0.9490 - val_loss: 0.1159 - val_accuracy: 0.9499\n",
      "Epoch 187/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0888 - accuracy: 0.9679 - val_loss: 0.1157 - val_accuracy: 0.9469\n",
      "Epoch 188/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1045 - accuracy: 0.9636 - val_loss: 0.1156 - val_accuracy: 0.9558\n",
      "Epoch 189/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.1062 - accuracy: 0.9665 - val_loss: 0.1182 - val_accuracy: 0.9558\n",
      "Epoch 190/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.0921 - accuracy: 0.9592 - val_loss: 0.1196 - val_accuracy: 0.9558\n",
      "Epoch 191/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1044 - accuracy: 0.9577 - val_loss: 0.1197 - val_accuracy: 0.9469\n",
      "Epoch 192/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.1027 - accuracy: 0.9650 - val_loss: 0.1177 - val_accuracy: 0.9410\n",
      "Epoch 193/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0977 - accuracy: 0.9679 - val_loss: 0.1125 - val_accuracy: 0.9499\n",
      "Epoch 194/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.0853 - accuracy: 0.9636 - val_loss: 0.1111 - val_accuracy: 0.9469\n",
      "Epoch 195/1000\n",
      "686/686 [==============================] - 0s 94us/sample - loss: 0.0959 - accuracy: 0.9606 - val_loss: 0.1099 - val_accuracy: 0.9617\n",
      "Epoch 196/1000\n",
      "686/686 [==============================] - 0s 99us/sample - loss: 0.1023 - accuracy: 0.9577 - val_loss: 0.1115 - val_accuracy: 0.9617\n",
      "Epoch 197/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0955 - accuracy: 0.9606 - val_loss: 0.1124 - val_accuracy: 0.9469\n",
      "Epoch 198/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0969 - accuracy: 0.9563 - val_loss: 0.1140 - val_accuracy: 0.9528\n",
      "Epoch 199/1000\n",
      "686/686 [==============================] - 0s 80us/sample - loss: 0.0968 - accuracy: 0.9621 - val_loss: 0.1117 - val_accuracy: 0.9558\n",
      "Epoch 200/1000\n",
      "686/686 [==============================] - 0s 80us/sample - loss: 0.0988 - accuracy: 0.9665 - val_loss: 0.1098 - val_accuracy: 0.9558\n",
      "Epoch 201/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0860 - accuracy: 0.9679 - val_loss: 0.1083 - val_accuracy: 0.9558\n",
      "Epoch 202/1000\n",
      "686/686 [==============================] - 0s 100us/sample - loss: 0.1044 - accuracy: 0.9606 - val_loss: 0.1071 - val_accuracy: 0.9617\n",
      "Epoch 203/1000\n",
      "686/686 [==============================] - 0s 108us/sample - loss: 0.0781 - accuracy: 0.9781 - val_loss: 0.1058 - val_accuracy: 0.9558\n",
      "Epoch 204/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.0879 - accuracy: 0.9636 - val_loss: 0.1063 - val_accuracy: 0.9558\n",
      "Epoch 205/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.0954 - accuracy: 0.9621 - val_loss: 0.1062 - val_accuracy: 0.9528\n",
      "Epoch 206/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.1108 - accuracy: 0.9723 - val_loss: 0.1068 - val_accuracy: 0.9528\n",
      "Epoch 207/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0907 - accuracy: 0.9679 - val_loss: 0.1099 - val_accuracy: 0.9499\n",
      "Epoch 208/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0864 - accuracy: 0.9606 - val_loss: 0.1089 - val_accuracy: 0.9558\n",
      "Epoch 209/1000\n",
      "686/686 [==============================] - 0s 103us/sample - loss: 0.0844 - accuracy: 0.9694 - val_loss: 0.1066 - val_accuracy: 0.9558\n",
      "Epoch 210/1000\n",
      "686/686 [==============================] - 0s 100us/sample - loss: 0.0834 - accuracy: 0.9636 - val_loss: 0.1060 - val_accuracy: 0.9558\n",
      "Epoch 211/1000\n",
      "686/686 [==============================] - 0s 96us/sample - loss: 0.0945 - accuracy: 0.9563 - val_loss: 0.1053 - val_accuracy: 0.9558\n",
      "Epoch 212/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0882 - accuracy: 0.9592 - val_loss: 0.1092 - val_accuracy: 0.9558\n",
      "Epoch 213/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.1028 - accuracy: 0.9563 - val_loss: 0.1072 - val_accuracy: 0.9558\n",
      "Epoch 214/1000\n",
      "686/686 [==============================] - 0s 121us/sample - loss: 0.0807 - accuracy: 0.9665 - val_loss: 0.1079 - val_accuracy: 0.9558\n",
      "Epoch 215/1000\n",
      "686/686 [==============================] - 0s 113us/sample - loss: 0.0954 - accuracy: 0.9694 - val_loss: 0.1053 - val_accuracy: 0.9646\n",
      "Epoch 216/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.0893 - accuracy: 0.9650 - val_loss: 0.1048 - val_accuracy: 0.9558\n",
      "Epoch 217/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0745 - accuracy: 0.9781 - val_loss: 0.1051 - val_accuracy: 0.9558\n",
      "Epoch 218/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0967 - accuracy: 0.9621 - val_loss: 0.1019 - val_accuracy: 0.9558\n",
      "Epoch 219/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0905 - accuracy: 0.9679 - val_loss: 0.1042 - val_accuracy: 0.9558\n",
      "Epoch 220/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1063 - accuracy: 0.9650 - val_loss: 0.1026 - val_accuracy: 0.9558\n",
      "Epoch 221/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.0832 - accuracy: 0.9679 - val_loss: 0.1024 - val_accuracy: 0.9558\n",
      "Epoch 222/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0776 - accuracy: 0.9723 - val_loss: 0.1021 - val_accuracy: 0.9558\n",
      "Epoch 223/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0746 - accuracy: 0.9723 - val_loss: 0.1015 - val_accuracy: 0.9646\n",
      "Epoch 224/1000\n",
      "686/686 [==============================] - 0s 88us/sample - loss: 0.0892 - accuracy: 0.9636 - val_loss: 0.1022 - val_accuracy: 0.9558\n",
      "Epoch 225/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.1120 - accuracy: 0.9519 - val_loss: 0.1054 - val_accuracy: 0.9558\n",
      "Epoch 226/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0930 - accuracy: 0.9592 - val_loss: 0.1030 - val_accuracy: 0.9558\n",
      "Epoch 227/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0819 - accuracy: 0.9708 - val_loss: 0.1030 - val_accuracy: 0.9617\n",
      "Epoch 228/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0848 - accuracy: 0.9723 - val_loss: 0.1045 - val_accuracy: 0.9528\n",
      "Epoch 229/1000\n",
      "686/686 [==============================] - 0s 80us/sample - loss: 0.0751 - accuracy: 0.9708 - val_loss: 0.1044 - val_accuracy: 0.9528\n",
      "Epoch 230/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0783 - accuracy: 0.9723 - val_loss: 0.1028 - val_accuracy: 0.9528\n",
      "Epoch 231/1000\n",
      "686/686 [==============================] - 0s 77us/sample - loss: 0.0924 - accuracy: 0.9592 - val_loss: 0.1011 - val_accuracy: 0.9558\n",
      "Epoch 232/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0780 - accuracy: 0.9752 - val_loss: 0.0978 - val_accuracy: 0.9646\n",
      "Epoch 233/1000\n",
      "686/686 [==============================] - 0s 80us/sample - loss: 0.0980 - accuracy: 0.9606 - val_loss: 0.0978 - val_accuracy: 0.9646\n",
      "Epoch 234/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0694 - accuracy: 0.9738 - val_loss: 0.0984 - val_accuracy: 0.9558\n",
      "Epoch 235/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0796 - accuracy: 0.9621 - val_loss: 0.0979 - val_accuracy: 0.9558\n",
      "Epoch 236/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.0849 - accuracy: 0.9665 - val_loss: 0.0979 - val_accuracy: 0.9558\n",
      "Epoch 237/1000\n",
      "686/686 [==============================] - 0s 115us/sample - loss: 0.0798 - accuracy: 0.9636 - val_loss: 0.0983 - val_accuracy: 0.9558\n",
      "Epoch 238/1000\n",
      "686/686 [==============================] - 0s 118us/sample - loss: 0.0882 - accuracy: 0.9708 - val_loss: 0.1004 - val_accuracy: 0.9558\n",
      "Epoch 239/1000\n",
      "686/686 [==============================] - 0s 94us/sample - loss: 0.0758 - accuracy: 0.9723 - val_loss: 0.0997 - val_accuracy: 0.9558\n",
      "Epoch 240/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0780 - accuracy: 0.9665 - val_loss: 0.1013 - val_accuracy: 0.9558\n",
      "Epoch 241/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.0896 - accuracy: 0.9723 - val_loss: 0.0974 - val_accuracy: 0.9558\n",
      "Epoch 242/1000\n",
      "686/686 [==============================] - 0s 96us/sample - loss: 0.0844 - accuracy: 0.9592 - val_loss: 0.0967 - val_accuracy: 0.9558\n",
      "Epoch 243/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.0759 - accuracy: 0.9694 - val_loss: 0.0963 - val_accuracy: 0.9646\n",
      "Epoch 244/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0963 - accuracy: 0.9694 - val_loss: 0.0942 - val_accuracy: 0.9705\n",
      "Epoch 245/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0959 - accuracy: 0.9636 - val_loss: 0.0962 - val_accuracy: 0.9646\n",
      "Epoch 246/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0944 - accuracy: 0.9621 - val_loss: 0.0978 - val_accuracy: 0.9646\n",
      "Epoch 247/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0830 - accuracy: 0.9636 - val_loss: 0.0992 - val_accuracy: 0.9646\n",
      "Epoch 248/1000\n",
      "686/686 [==============================] - 0s 96us/sample - loss: 0.0870 - accuracy: 0.9636 - val_loss: 0.1001 - val_accuracy: 0.9617\n",
      "Epoch 249/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0764 - accuracy: 0.9650 - val_loss: 0.0988 - val_accuracy: 0.9558\n",
      "Epoch 250/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0832 - accuracy: 0.9650 - val_loss: 0.1023 - val_accuracy: 0.9558\n",
      "Epoch 251/1000\n",
      "686/686 [==============================] - 0s 96us/sample - loss: 0.0977 - accuracy: 0.9636 - val_loss: 0.1013 - val_accuracy: 0.9558\n",
      "Epoch 252/1000\n",
      "686/686 [==============================] - 0s 97us/sample - loss: 0.0853 - accuracy: 0.9679 - val_loss: 0.1021 - val_accuracy: 0.9558\n",
      "Epoch 253/1000\n",
      "686/686 [==============================] - 0s 93us/sample - loss: 0.0797 - accuracy: 0.9708 - val_loss: 0.1007 - val_accuracy: 0.9558\n",
      "Epoch 254/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0696 - accuracy: 0.9752 - val_loss: 0.1020 - val_accuracy: 0.9558\n",
      "Epoch 255/1000\n",
      "686/686 [==============================] - 0s 94us/sample - loss: 0.0733 - accuracy: 0.9708 - val_loss: 0.1018 - val_accuracy: 0.9558\n",
      "Epoch 256/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0863 - accuracy: 0.9636 - val_loss: 0.1021 - val_accuracy: 0.9558\n",
      "Epoch 257/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0669 - accuracy: 0.9767 - val_loss: 0.0976 - val_accuracy: 0.9558\n",
      "Epoch 258/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0807 - accuracy: 0.9708 - val_loss: 0.0983 - val_accuracy: 0.9558\n",
      "Epoch 259/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0714 - accuracy: 0.9767 - val_loss: 0.0982 - val_accuracy: 0.9617\n",
      "Epoch 260/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0864 - accuracy: 0.9694 - val_loss: 0.0972 - val_accuracy: 0.9676\n",
      "Epoch 261/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0783 - accuracy: 0.9708 - val_loss: 0.0969 - val_accuracy: 0.9676\n",
      "Epoch 262/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0627 - accuracy: 0.9825 - val_loss: 0.0966 - val_accuracy: 0.9617\n",
      "Epoch 263/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0763 - accuracy: 0.9752 - val_loss: 0.0913 - val_accuracy: 0.9617\n",
      "Epoch 264/1000\n",
      "686/686 [==============================] - 0s 120us/sample - loss: 0.0894 - accuracy: 0.9665 - val_loss: 0.0894 - val_accuracy: 0.9676\n",
      "Epoch 265/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0715 - accuracy: 0.9767 - val_loss: 0.0903 - val_accuracy: 0.9676\n",
      "Epoch 266/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0996 - accuracy: 0.9621 - val_loss: 0.0905 - val_accuracy: 0.9705\n",
      "Epoch 267/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0869 - accuracy: 0.9665 - val_loss: 0.0900 - val_accuracy: 0.9735\n",
      "Epoch 268/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0939 - accuracy: 0.9738 - val_loss: 0.0931 - val_accuracy: 0.9735\n",
      "Epoch 269/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0663 - accuracy: 0.9723 - val_loss: 0.0934 - val_accuracy: 0.9705\n",
      "Epoch 270/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0655 - accuracy: 0.9723 - val_loss: 0.0947 - val_accuracy: 0.9705\n",
      "Epoch 271/1000\n",
      "686/686 [==============================] - 0s 94us/sample - loss: 0.0800 - accuracy: 0.9636 - val_loss: 0.0967 - val_accuracy: 0.9705\n",
      "Epoch 272/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0961 - accuracy: 0.9694 - val_loss: 0.0974 - val_accuracy: 0.9705\n",
      "Epoch 273/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0743 - accuracy: 0.9796 - val_loss: 0.0994 - val_accuracy: 0.9617\n",
      "Epoch 274/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1021 - accuracy: 0.9665 - val_loss: 0.0965 - val_accuracy: 0.9617\n",
      "Epoch 275/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0773 - accuracy: 0.9665 - val_loss: 0.0943 - val_accuracy: 0.9617\n",
      "Epoch 276/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0760 - accuracy: 0.9665 - val_loss: 0.0938 - val_accuracy: 0.9617\n",
      "Epoch 277/1000\n",
      "686/686 [==============================] - 0s 85us/sample - loss: 0.0823 - accuracy: 0.9738 - val_loss: 0.0966 - val_accuracy: 0.9735\n",
      "Epoch 278/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0663 - accuracy: 0.9810 - val_loss: 0.0933 - val_accuracy: 0.9735\n",
      "Epoch 279/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0818 - accuracy: 0.9723 - val_loss: 0.0914 - val_accuracy: 0.9735\n",
      "Epoch 280/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0613 - accuracy: 0.9781 - val_loss: 0.0938 - val_accuracy: 0.9646\n",
      "Epoch 281/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0726 - accuracy: 0.9781 - val_loss: 0.0898 - val_accuracy: 0.9735\n",
      "Epoch 282/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0778 - accuracy: 0.9694 - val_loss: 0.0882 - val_accuracy: 0.9646\n",
      "Epoch 283/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0778 - accuracy: 0.9752 - val_loss: 0.0883 - val_accuracy: 0.9646\n",
      "Epoch 284/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0729 - accuracy: 0.9679 - val_loss: 0.0871 - val_accuracy: 0.9646\n",
      "Epoch 285/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0771 - accuracy: 0.9781 - val_loss: 0.0886 - val_accuracy: 0.9735\n",
      "Epoch 286/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0884 - accuracy: 0.9665 - val_loss: 0.0899 - val_accuracy: 0.9735\n",
      "Epoch 287/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0533 - accuracy: 0.9869 - val_loss: 0.0871 - val_accuracy: 0.9735\n",
      "Epoch 288/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0492 - accuracy: 0.9869 - val_loss: 0.0866 - val_accuracy: 0.9735\n",
      "Epoch 289/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0758 - accuracy: 0.9767 - val_loss: 0.0893 - val_accuracy: 0.9735\n",
      "Epoch 290/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0738 - accuracy: 0.9708 - val_loss: 0.0877 - val_accuracy: 0.9735\n",
      "Epoch 291/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0796 - accuracy: 0.9723 - val_loss: 0.0871 - val_accuracy: 0.9735\n",
      "Epoch 292/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.0757 - accuracy: 0.9708 - val_loss: 0.0887 - val_accuracy: 0.9735\n",
      "Epoch 293/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0851 - accuracy: 0.9636 - val_loss: 0.0912 - val_accuracy: 0.9705\n",
      "Epoch 294/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0754 - accuracy: 0.9665 - val_loss: 0.0877 - val_accuracy: 0.9735\n",
      "Epoch 295/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0655 - accuracy: 0.9767 - val_loss: 0.0876 - val_accuracy: 0.9735\n",
      "Epoch 296/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0740 - accuracy: 0.9708 - val_loss: 0.0853 - val_accuracy: 0.9705\n",
      "Epoch 297/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0806 - accuracy: 0.9708 - val_loss: 0.0849 - val_accuracy: 0.9705\n",
      "Epoch 298/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0718 - accuracy: 0.9665 - val_loss: 0.0857 - val_accuracy: 0.9735\n",
      "Epoch 299/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0553 - accuracy: 0.9825 - val_loss: 0.0863 - val_accuracy: 0.9735\n",
      "Epoch 300/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0754 - accuracy: 0.9708 - val_loss: 0.0840 - val_accuracy: 0.9735\n",
      "Epoch 301/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0848 - accuracy: 0.9723 - val_loss: 0.0825 - val_accuracy: 0.9823\n",
      "Epoch 302/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0763 - accuracy: 0.9752 - val_loss: 0.0826 - val_accuracy: 0.9823\n",
      "Epoch 303/1000\n",
      "686/686 [==============================] - 0s 92us/sample - loss: 0.1000 - accuracy: 0.9592 - val_loss: 0.0835 - val_accuracy: 0.9823\n",
      "Epoch 304/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0681 - accuracy: 0.9767 - val_loss: 0.0855 - val_accuracy: 0.9735\n",
      "Epoch 305/1000\n",
      "686/686 [==============================] - 0s 86us/sample - loss: 0.0787 - accuracy: 0.9752 - val_loss: 0.0879 - val_accuracy: 0.9735\n",
      "Epoch 306/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0870 - val_accuracy: 0.9735\n",
      "Epoch 307/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0840 - accuracy: 0.9723 - val_loss: 0.0911 - val_accuracy: 0.9735\n",
      "Epoch 308/1000\n",
      "686/686 [==============================] - 0s 82us/sample - loss: 0.0607 - accuracy: 0.9810 - val_loss: 0.0908 - val_accuracy: 0.9735\n",
      "Epoch 309/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0646 - accuracy: 0.9738 - val_loss: 0.0890 - val_accuracy: 0.9646\n",
      "Epoch 310/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0527 - accuracy: 0.9796 - val_loss: 0.0879 - val_accuracy: 0.9646\n",
      "Epoch 311/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0819 - accuracy: 0.9708 - val_loss: 0.0895 - val_accuracy: 0.9735\n",
      "Epoch 312/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0556 - accuracy: 0.9810 - val_loss: 0.0876 - val_accuracy: 0.9823\n",
      "Epoch 313/1000\n",
      "686/686 [==============================] - 0s 81us/sample - loss: 0.0793 - accuracy: 0.9708 - val_loss: 0.0894 - val_accuracy: 0.9735\n",
      "Epoch 314/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0667 - accuracy: 0.9767 - val_loss: 0.0917 - val_accuracy: 0.9676\n",
      "Epoch 315/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0736 - accuracy: 0.9723 - val_loss: 0.0909 - val_accuracy: 0.9676\n",
      "Epoch 316/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0712 - accuracy: 0.9781 - val_loss: 0.0912 - val_accuracy: 0.9705\n",
      "Epoch 317/1000\n",
      "686/686 [==============================] - 0s 89us/sample - loss: 0.0670 - accuracy: 0.9723 - val_loss: 0.0923 - val_accuracy: 0.9617\n",
      "Epoch 318/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0624 - accuracy: 0.9694 - val_loss: 0.0949 - val_accuracy: 0.9617\n",
      "Epoch 319/1000\n",
      "686/686 [==============================] - 0s 94us/sample - loss: 0.0667 - accuracy: 0.9752 - val_loss: 0.0928 - val_accuracy: 0.9617\n",
      "Epoch 320/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0714 - accuracy: 0.9694 - val_loss: 0.0891 - val_accuracy: 0.9735\n",
      "Epoch 321/1000\n",
      "686/686 [==============================] - 0s 103us/sample - loss: 0.0601 - accuracy: 0.9767 - val_loss: 0.0854 - val_accuracy: 0.9735\n",
      "Epoch 322/1000\n",
      "686/686 [==============================] - 0s 96us/sample - loss: 0.0709 - accuracy: 0.9738 - val_loss: 0.0856 - val_accuracy: 0.9735\n",
      "Epoch 323/1000\n",
      "686/686 [==============================] - 0s 90us/sample - loss: 0.0767 - accuracy: 0.9810 - val_loss: 0.0845 - val_accuracy: 0.9735\n",
      "Epoch 324/1000\n",
      "686/686 [==============================] - 0s 87us/sample - loss: 0.0607 - accuracy: 0.9810 - val_loss: 0.0835 - val_accuracy: 0.9735\n",
      "Epoch 325/1000\n",
      "686/686 [==============================] - 0s 83us/sample - loss: 0.0722 - accuracy: 0.9679 - val_loss: 0.0866 - val_accuracy: 0.9735\n",
      "Epoch 326/1000\n",
      "686/686 [==============================] - 0s 84us/sample - loss: 0.0590 - accuracy: 0.9810 - val_loss: 0.0859 - val_accuracy: 0.9735\n",
      "Epoch 00326: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27e7a6b7a08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the Dense layers and activation functions\n",
    "\n",
    "nn = keras.Sequential()\n",
    "\n",
    "nn.add(Dense(30, activation='relu'))\n",
    "\n",
    "nn.add(Dropout(0.2))\n",
    "\n",
    "nn.add(Dense(15, activation='relu'))\n",
    "\n",
    "nn.add(Dropout(0.2))\n",
    "\n",
    "nn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',mode='min', verbose=1, patience=25)\n",
    "\n",
    "nn.fit(X_train, y_train, epochs = 1000, validation_data=(X_test, y_test),\n",
    "         callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model loss\n",
    "\n",
    "#model_loss = pd.DataFrame(nn.history.history)\n",
    "#model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.evaluate(X_train, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import math\n",
    "\n",
    "#math.sqrt(mean_squared_error(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_absolute_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       156\n",
      "           1       0.98      0.97      0.98       183\n",
      "\n",
      "    accuracy                           0.97       339\n",
      "   macro avg       0.97      0.97      0.97       339\n",
      "weighted avg       0.97      0.97      0.97       339\n",
      "\n",
      "[[153   3]\n",
      " [  6 177]]\n"
     ]
    }
   ],
   "source": [
    "#test the accuracy of the network\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new = [[52,1,0,125,212,0,1,168,0,1.0,2,2,3]]\n",
    "#X_new = scaler.transform(X_new)\n",
    "#nn.predict_classes(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new2 = [[59,1,1,140,221,0,1,164,1,0.0,2,0,2]]\n",
    "#X_new2 = scaler.transform(X_new2)\n",
    "#nn.predict_classes(X_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save(\"heart_disease_model_bry.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dtype(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heart_scaler_bry.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, 'heart_scaler_bry.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "heart_model = load_model(\"heart_disease_model_bry.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_scaler = joblib.load(\"heart_scaler_bry.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_example = {\"age\":52,\n",
    "                  \"sex\":1,\n",
    "                 \"cp\":0,\n",
    "                 \"trestbps\":125,\n",
    "                 \"chol\":212,\n",
    "                \"fbs\":0,\n",
    "                \"restecg\":1,\n",
    "                \"thalach\":168,\n",
    "                \"exang\":0,\n",
    "                \"oldpeak\":1.0,\n",
    "                \"slope\":2,\n",
    "                \"ca\":2,\n",
    "                \"thal\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(model, scaler, sample_json):\n",
    "    \n",
    "    age = sample_json[\"age\"]\n",
    "    sex = sample_json[\"sex\"]\n",
    "    cp = sample_json['cp']\n",
    "    trestbps = sample_json['trestbps']\n",
    "    chol = sample_json['chol']\n",
    "    fbs = sample_json['fbs']\n",
    "    restecg = sample_json['restecg']\n",
    "    thalach = sample_json['thalach']\n",
    "    exang = sample_json['exang']\n",
    "    oldpeak = sample_json['oldpeak']\n",
    "    slope = sample_json['slope']\n",
    "    ca = sample_json['ca']\n",
    "    thal = sample_json['thal']\n",
    "\n",
    "                        \n",
    "    \n",
    "    disease = [[age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal]]\n",
    "    \n",
    "    classes = np.array(['0', '1'])\n",
    "    \n",
    "    disease = scaler.transform(disease)\n",
    "    \n",
    "    class_ind = model.predict_classes(disease)\n",
    "    \n",
    "    return classes[class_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0']], dtype='<U1')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_prediction(nn, heart_scaler, heart_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
